---
title: "Tekst Analyse"
output: html_notebook
---

### Libraries laden

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
library(gsheet)
library(tidyverse)
library(udpipe)
library(wordcloud2)
library(tidytext)
library(tm)
library(quanteda)
library(topicmodels)
library(stopwords)
library(plotly)
```

### Data inlezen

```{r}
url <- "https://docs.google.com/spreadsheets/d/1Ps_aDZdcU2HOc6qwzXKtdS0jcpXCgLESVeiC6JN_VEk/edit?usp=sharing"
rawDF <- gsheet2tbl(url)
head(rawDF)
str(rawDF)
```
### Tekst annoteren
```{r}
udmodel <- udpipe_download_model(language = "dutch", overwrite = FALSE)
udmodel_dutch <- udpipe_load_model(file = udmodel$file_model)
fltDF <- na.omit(tolower(rawDF[[19]]))
annData <- udpipe_annotate(udmodel_dutch, x = fltDF)
annDF <- as.data.frame(annData)
str(annDF)
head(annDF)
```

### Data filteren en wordcloud bouwen
```{r}
annDFflt <- filter(annDF, upos %in% c("ADJ", "VERB", "NOUN"))
by_upos <- group_by(annDFflt, token)
wordFreq <- summarise(by_upos,
  freq = n()
)
## Verwijder woorden met een neutrale betekenis en toon alleen de woorden die vaker dan 10 keer voorkomen.
wordFreq <- filter(wordFreq, !(token %in% c("kunnen", "zien", "beter", "kan", "staat", "staan", "komen", "moet")), freq > 3)
wordcloud2(data = wordFreq, size = 1)
```

### Colocaties bepalen
```{r}
## Collocation (words following one another)
stats1 <- keywords_collocation(x = annDF, 
                             term = "token", group = c("doc_id", "paragraph_id", "sentence_id"),
                             ngram_max = 4)
## Co-occurrences: How frequent do words occur in the same sentence, in this case only nouns or adjectives
stats2 <- cooccurrence(x = subset(x = annDF, upos %in% c("NOUN", "ADJ")), 
                     term = "lemma", group = c("doc_id", "paragraph_id", "sentence_id"))
## Co-occurrences: How frequent do words follow one another
stats3 <- cooccurrence(x = annDF$lemma, 
                     relevant = annDF$upos %in% c("NOUN", "ADJ"))
## Co-occurrences: How frequent do words follow one another even if we would skip 2 words in between
stats4 <- cooccurrence(x = annDF$lemma, 
                     relevant = annDF$upos %in% c("NOUN", "ADJ"), skipgram = 2)
head(data.frame(stats3))
```

### Topics bepalen

Met topic modeling kan gekeken worden of de reacties in de open vraag gegroepeerd kunnen worden naar thema. In dit geval hebben we de opdracht gegeven de antwoorden in twee groepen op te delen zodanig dat ze zoveel mogelijk van elkaar verschillen.

Hiervoor wordt per woord/topic-combinatie een parameter berekend, de beta. Hoe hoger de beta des te meer associatie het woord met een topic heeft. Door per woord te filteren op de hoogste beta, ontstaan twee woordgroepen.

```{r}
stop_words <- data.frame(word = c(stopwords(language = "nl", source = "snowball"), "niks", "oo", "onderwijs", "online"))

corpusDF <- unique(select(annDF, doc_id, sentence))

doc_words <- corpusDF %>%
  unnest_tokens(word, sentence) %>%
  count(doc_id, word, sort = TRUE) %>%
  ungroup() %>%
  anti_join(stop_words)

total_words <- doc_words %>% 
  group_by(doc_id) %>% 
  summarize(total = sum(n))

doc_words <- left_join(doc_words, total_words)

doc_words

doc_term_mat <- doc_words %>% cast_dtm(doc_id, word, n)
doc_lda <- LDA(doc_term_mat, k = 2, control = list(seed = 1234))
doc_lda

doc_topics <- tidy(doc_lda, matrix = "beta")
doc_topics
```

```{r}
doc_top_terms <- doc_topics %>%
  group_by(topic) %>%
  top_n(15, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

doc_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```



